{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00307ce",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Anomaly Detection with Negative Selection\n",
    "\n",
    "### Project description:\n",
    "\n",
    "In this project, you will use the same Textor algorithm from the previous project to detect anomalies in a different data set. We will use a fetal monitoring dataset (known as a cardiotocography) from the UC Irvine Machine Learning Repository. \n",
    "\n",
    "### Project goals:\n",
    "\n",
    "1. Apply the Textor algorithm to a practical example\n",
    "2. Use ROC analysis to score the detector\n",
    "\n",
    "### Project question overview:\n",
    "\n",
    "1. Write a function to process the cardiotocography data set. [Question 1](#question1)\n",
    "2. Paste any previous functions or write additional helper functions (not-graded). [Question 2](#question2)\n",
    "3. Write a function to calculate AUC values over an interval of $r$ values. [Question 3](#question3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5a0c9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Cardiotocography Dataset\n",
    "\n",
    "A cardiotocography is a technical way to measure the fetal heart rate (FHR) and the uterine contractions (UC) during pregnancy. Obstetricians then classify these readings as either normal, suspect, and pathologic. Figure 1 shows the display of a cardiotocograph (CTG), the FHR is shown in orange and the UC is shown in green. Figure 2 shows the output of a typical CTG where the line labeled **A** is the FHR and the line labeled **D** is the UC. More details on the data set are here: http://odds.cs.stonybrook.edu/cardiotocogrpahy-dataset/. \n",
    "\n",
    "The data consists of 21 real-valued variables, outliers, and inliers. Each row is a sample, each column is an observed variable, and the training file includes the ground truth labels of the samples. The data are in: ``cardio_train.csv`` and ``cardio_test.csv``. \n",
    "\n",
    "<table><tr>\n",
    "<td> \n",
    "  <p align=\"center\" style=\"padding: 10px\">\n",
    "    <img alt=\"Forwarding\" src=\"images/cardio.jpg\" width=\"250\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">Figure 1: Cardiotocograph display</em>\n",
    "  </p> \n",
    "</td>\n",
    "<td> \n",
    "  <p align=\"center\">\n",
    "    <img alt=\"Routing\" src=\"images/CTG_Output.jpg\" width=\"650\">\n",
    "    <br>\n",
    "    <em style=\"color: grey\">Figure 2: Cardiotocograph output</em>\n",
    "  </p> \n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "*Image sources: Wikipedia*\n",
    "\n",
    "\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "In order for the continuous data to work well with the negative selection algorithm, we will need to bin the values into categorical variables. For consistency across assignments, we will simply bin each of the variables into 10 bins. This will produce strings of length 21 symbols and an alphabet size of 10 characters (e.g. A indicates the lowest category and J indicates the highest). \n",
    "\n",
    "Next, we will need to format the data in such a way that the new train and test data may be read directly by Textor's algorithm used in the previous project. Recall that both the train and the test data for the languages were ``.csv`` files with one string of length 10 per line. Here, we would like there to be one string of length 21 per line. These strings will correspond to the 21 variables in a row of either ``cardio_train.csv`` or ``cardio_test.csv``. \n",
    "\n",
    "### Creating Bins with pandas\n",
    "\n",
    "To avoid ambiguity in how the data can be binned, we suggest that you use the ``cut`` function from pandas. Note that this function allows for a label parameter to be passed. To map our binned data to characters we can define the labels array as follows:\n",
    "\n",
    "``labels=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']``.\n",
    "\n",
    "After defining labels we can call ``pandas.cut(x, bins=10, labels)``, where ``x`` is the data column.\n",
    "\n",
    "<a id='question1'></a>\n",
    "# Question 1\n",
    "\n",
    "Write the function ``process_data(file)`` which processes either ``cardio_train.csv`` or ``cardio_test.csv`` into the format specified in the \"Data Processing\" section. This function returns the string ``processed_file`` which is a path to the final processed file. This function will not be directly graded but will be used in the grading of later questions. The processed file should be in ``.csv`` format with one string of length 21 per row. The final processed file should look similar to the strings below. \n",
    "\n",
    "**Do not include headers or indices in your final processed file. The file should contain only strings.**\n",
    "\n",
    "```\n",
    "EEDAECEAACACIBJDBEDDA\n",
    "EEBAFCEAACACIBJCBEDDA\n",
    "EEBAFBEAADAEHAFGAEDDA\n",
    "EEDAFAEAADADHAFFAEDDA\n",
    "EECBFBEACBACDDDCAEDDA\n",
    "EEEEEAEACBABFBEBAEDEA\n",
    "DDDICDECBCABGBGDAFCDE\n",
    "DDCHCCEEBCABGBGBAECDC\n",
    "DDDJDCEABCACIAHCAFCDD\n",
    "DDBJDCEFBCABGAEDADCCC\n",
    "DDCJDCEDCBABHBGDBDCDB\n",
    "DDCICCEFCBABJAJFADCCB\n",
    "EEACAHEACIAAJAJFADAAF\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "da08600a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "import numpy as np\n",
    "from subprocess import run, PIPE, TimeoutExpired\n",
    "from sklearn.metrics import auc\n",
    "import pandas as pd\n",
    "\n",
    "def process_data(file):\n",
    "    \"\"\"\n",
    "    Process data from CSV into binned strings of length 21 using pure Python.\n",
    "    \"\"\"\n",
    "    labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    processed_rows = []\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    # Iterate over rows by name\n",
    "    for index, row in df.iterrows():\n",
    "        # Access the column's values\n",
    "        row_values = row[1:22]\n",
    "        binned_row = pd.cut(row_values, bins=10, labels=labels, right=False)\n",
    "        processed_rows.append(''.join(binned_row))  \n",
    "\n",
    "    # # Read file\n",
    "    # with open(file, 'r') as f:\n",
    "    #     reader = csv.reader(f)\n",
    "    #     for row in reader:\n",
    "    #         try:\n",
    "    #             # Convert all values in the row to floats\n",
    "    #             row = [float(value) for value in row]\n",
    "    #             row = row[1:]\n",
    "    #             # Bin each column value into one of 10 categories\n",
    "    #             binned_row = pd.cut(row, bins=10, labels=labels)\n",
    "    #             processed_rows.append(''.join(binned_row))  # Ensure 21 characters per line\n",
    "    #         except ValueError:\n",
    "    #             # Skip rows that cannot be converted to floats\n",
    "    #             print(f\"Skipping invalid row: {row}\")\n",
    "    #             continue\n",
    "\n",
    "    # Save processed file\n",
    "    processed_file = file.replace('.csv', '_processed.csv')\n",
    "    with open(processed_file, 'w') as f:\n",
    "        for line in processed_rows:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    print(f\"Processed file saved as: {processed_file} with {len(processed_rows)} rows.\")\n",
    "    return processed_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb84402",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Implementing Negative Selection Recap\n",
    "\n",
    "Recall from the previous project that we used a Python subprocess to use Textor's ``negsel.jar`` file. To start a subprocess, we will first need to import the ``subprocess`` module and the ``run`` and ``PIPE`` submodules. Then, we will need to run the process with an array of arguments and the opened input file. We will use the Textor example ([here](http://johannes-textor.name/negativeselection.html)) to demonstrate how subprocesses work.\n",
    "\n",
    "The following command trains the negative selection algorithm on the ``english.train`` data set and tests on the ``hiligaynon.test`` data set from the previous project.\n",
    "\n",
    "``java -jar negsel.jar -c -n 10 -r 4 -self english.train < hiligaynon.test ``\n",
    "\n",
    "In this project, you will use the processed train and test data to run this same command. Recall that to run this subprocess, we will need to first build an argument list from the command. This allows the subprocess command to parse the command and flags. Using the command given above, we will construct the following argument list. \n",
    "\n",
    "``args = ['java', '-jar', 'negsel.jar', '-c', '-n', '10', '-r','4', '-self', 'negsel_src/tests/english.train']``\n",
    "\n",
    "Next, we will need to open the test file to read from. This step is necessary as the argument list will not recognize the redirection from stdin (``< hiligaynon.test``).\n",
    "\n",
    "``input_file = open('negsel_src/tests/hiligaynon.test', 'r')``\n",
    "\n",
    "Finally, we can use the ``run`` submodule to call the command and pipe the output. \n",
    "\n",
    "``result = run(args, stdout=PIPE, stdin=input_file, universal_newlines=True)``\n",
    "\n",
    "The final results will be stored in ``result.stdout``. This can be saved and parsed later as needed. \n",
    "\n",
    "Refer to the previous project for more information on how to run the code as well as a full running example.\n",
    "\n",
    "**REMEMBER TO ADJUST THE STRING LENGTH FOR THIS PROJECT.**\n",
    "\n",
    "## Exploring Model Parameterization\n",
    "\n",
    "In the previous project, we used $r=4$ contiguous bits for all of our testing. However, this does not ensure optimal anomaly detection. In this project, we will investigate the tuning of the $r$ parameter for the *$r$-contiguous* patterns. \n",
    " \n",
    "\n",
    "### $r$-contiguous Patterns\n",
    "\n",
    "Textor in _\"A Comparative Study of Negative Selection Based Anomaly Detection in Sequence Data\"_ defines the $r$-contiguous pattern as follows.\n",
    "\n",
    "<blockquote>An $r$-contiguous pattern is a string $d\\in \\Sigma^l$. It matches another string $s\\in  \\Sigma^l$ if $d$ and $s$ are identical in at least $r$ contiguous positions, i.e., if there is an $i\\in \\{1,...,l-r+1\\}$ such that the substrings of length $r$ of $s$ and $d$ starting at the $i$-th position are equal. </blockquote> \n",
    "\n",
    "### Scoring the Detector \n",
    "\n",
    "Since our strings are of length 21, we will consider $r\\in [2,10]$. For simplicity, we will use the *AUC* as described in the previous project. Recall that the ROC curve is created by plotting the false positive (FP) rate and the true positive (TP) rate over an interval of values of $\\theta$. Generally, a meaningful classifier has an area under the curve (AUC) value greater than 0.5, and an AUC value close to 1 signifies a near-perfect classifier. \n",
    "\n",
    "You may use the ``fp_tp_calc(file, theta)`` and ``auc_calc(file)`` functions from the previous project to calculate the AUC values. Additionally, we will let $\\theta \\in [0,10]$, and $ \\theta \\in \\mathbb{Z}$. \n",
    "\n",
    "<a id='question2'></a>\n",
    "# Question 2: Using previous functions\n",
    "\n",
    "Use the answer box below to paste in any functions that you would like to use for Question 2. You may also use this area to write any additional helper functions and/or edit your old functions to generalize to this problem and dataset (i.e., by adding a new parameter to handle the fact that r is an additional input variable). Labels for this dataset may be found in the file cardio.labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "43a5d998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved as: cardio_train_processed.csv with 1000 rows.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import subprocess\n",
    "\n",
    "train_file = process_data('cardio_train.csv')\n",
    "\n",
    "# Step 2: FP and TP calculation\n",
    "def fp_tp_calc( file, theta, r):\n",
    "    args = ['java', '-jar', 'negsel.jar', '-c', '-n', '21', '-r', str(r), '-self', train_file]\n",
    "    result = subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=file, universal_newlines=True)\n",
    "    scores = [float(line) for line in result.stdout.strip().splitlines()]\n",
    "\n",
    "    labels = pd.read_csv('cardio.labels', header=None).squeeze().tolist()\n",
    "    \n",
    "    tp = sum(1 for i in range(len(scores)) if scores[i] > theta and labels[i] == 1)\n",
    "    fn = sum(1 for i in range(len(scores)) if scores[i] <= theta and labels[i] == 1)\n",
    "    fp = sum(1 for i in range(len(scores)) if scores[i] > theta and labels[i] == 0)\n",
    "    tn = sum(1 for i in range(len(scores)) if scores[i] <= theta and labels[i] == 0)\n",
    "    \n",
    "    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "    tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    return (fpr, tpr)\n",
    "\n",
    "def auc_calc(file, r):\n",
    "    theta_range = range(0, 11)\n",
    "    print(\"calculating value for r:\" + str(r))\n",
    "    input_file = open(process_data(file), 'r')\n",
    "    fpr_tpr = [fp_tp_calc(input_file, theta, r) for theta in theta_range]\n",
    "    \n",
    "    fprs, tprs = zip(*fpr_tpr)\n",
    "    auc = metrics.auc(fprs, tprs)\n",
    "    return round(auc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14f923",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id='question3'></a>\n",
    "# Question 3\n",
    "\n",
    "Write the function ``r_auc_tuple(file)`` that calculates a list of tuples for each value of $r\\in [2,10]$ over $\\theta \\in [0,10]$ for a given test file. This will result in a list of 10 tuples where the first element in each tuple is the value of $r$ and the second element is the AUC value.  This list will be returned as ``tuples_list`` and will be graded based on accuracy. The labels for the test file are in ``cardio.labels``.\n",
    "\n",
    "For reference, the following command should return the AUC value for $r=2$: ``r_auc_tuple('cardio_test.csv')[0][1]``.\n",
    "\n",
    "**Remember to use the ``process_data(file)`` function to run the Textor algorithm with the correct files.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3f4574e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_auc_tuple(file):\n",
    "    \"\"\"\n",
    "    Calculate AUC values for various values of r (2 to 10) for a given test file.\n",
    "    \"\"\"\n",
    "    # train_file = process_data('cardio_train.csv')\n",
    "    # test_file = process_data(file)\n",
    "\n",
    "    # # Read labels\n",
    "    # with open('cardio.labels', 'r') as f:\n",
    "    #     labels = [int(line.strip()) for line in f]\n",
    "\n",
    "    # # Debugging: Check lengths of test file and labels\n",
    "    # print(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "    tuples_list = []\n",
    "\n",
    "    for r in range(2, 5):\n",
    "        # timeout = 120\n",
    "        # args = ['java', '-jar', 'negsel.jar', '-c', '-n', '10', '-r', str(r), '-self', train_file]\n",
    "\n",
    "        # with open(test_file, 'r') as input_file:\n",
    "        #     try:\n",
    "        #         result = run(args, stdout=PIPE, stdin=input_file, universal_newlines=True, timeout=timeout)\n",
    "        #     except TimeoutExpired:\n",
    "        #         print(f\"Timeout expired for r={r}, skipping to the next value.\")\n",
    "        #         continue\n",
    "\n",
    "        # # Parse scores from negsel.jar output\n",
    "        # scores = []\n",
    "        # for line in result.stdout.splitlines():\n",
    "        #     for value in line.split():\n",
    "        #         try:\n",
    "        #             scores.append(float(value))\n",
    "        #         except ValueError:\n",
    "        #             continue\n",
    "\n",
    "        # # Debugging: Check scores\n",
    "        # print(f\"Scores for r = {r}: {scores[:10]} (first 10 scores)\")\n",
    "\n",
    "        # if not scores:\n",
    "        #     print(f\"No scores generated for r = {r}\")\n",
    "        #     continue\n",
    "\n",
    "        # # Normalize scores\n",
    "        # max_score = max(scores)\n",
    "        # normalized_scores = [(score / max_score) * 50 for score in scores] if max_score > 0 else [0] * len(scores)\n",
    "\n",
    "        # # Debugging: Check normalized scores\n",
    "        # print(f\"Normalized Scores for r = {r}: {normalized_scores[:10]} (first 10 normalized)\")\n",
    "\n",
    "        # # Ensure scores and labels align\n",
    "        # effective_length = min(len(normalized_scores), len(labels))\n",
    "        # normalized_scores = normalized_scores[:effective_length]\n",
    "        # labels = labels[:effective_length]\n",
    "\n",
    "        # # Debugging: Check alignment\n",
    "        # print(f\"Effective lengths for r = {r}: {effective_length}\")\n",
    "\n",
    "        # # Calculate FP-TP values\n",
    "        # theta_range = np.arange(0, 11, 1)\n",
    "        # fp_tp_values = [fp_tp_calc(normalized_scores, theta, labels) for theta in theta_range]\n",
    "\n",
    "        # # Debugging: Check FP-TP values\n",
    "        # print(f\"FP-TP Values for r = {r}: {fp_tp_values}\")\n",
    "\n",
    "        # if len(fp_tp_values) < 2:\n",
    "        #     print(f\"Insufficient FP-TP values for r = {r}, skipping.\")\n",
    "        #     continue\n",
    "\n",
    "        # Calculate AUC\n",
    "    \n",
    "        auc_value = auc_calc(file, r)\n",
    "        tuples_list.append((r, auc_value))\n",
    "       \n",
    "\n",
    "    return tuples_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0f3cb7c0-5a16-49c9-8748-8616915bd7ab",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating value for r:2\n",
      "Processed file saved as: cardio_test_processed.csv with 831 rows.\n",
      "calculating value for r:3\n",
      "Processed file saved as: cardio_test_processed.csv with 831 rows.\n",
      "calculating value for r:4\n",
      "Processed file saved as: cardio_test_processed.csv with 831 rows.\n",
      "[(2, np.float64(0.0)), (3, np.float64(0.42)), (4, np.float64(0.5))]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(file)\n\u001b[1;32m     13\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterows(tuples)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tuples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mround\u001b[39m(tuples[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.75\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mround\u001b[39m(tuples[\u001b[38;5;241m7\u001b[39m][\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "You must run this cell.\n",
    "Check the output of your functions.\n",
    "'''\n",
    "import csv\n",
    "\n",
    "tuples = r_auc_tuple('cardio_test.csv')\n",
    "print(tuples)\n",
    "\n",
    "# don't edit the below code\n",
    "with open('tuples.csv','w',newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(tuples)\n",
    "\n",
    "assert len(tuples) == 9\n",
    "assert round(tuples[0][1],2) == 0.75\n",
    "assert round(tuples[7][1],2) == 0.0\n",
    "print(\"All test cases passed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
